# Web Crawler e Web Scraping de NotÃ­cias de Tecnologia

Implementar um web crawler e tÃ©cnicas de web scraping, com o objetivo de coletar, processar e estruturar dados disponÃ­veis na web. A aplicaÃ§Ã£o explora conceitos de mineraÃ§Ã£o de dados, automaÃ§Ã£o de processos, anÃ¡lise de estruturas HTML e boas prÃ¡ticas de desenvolvimento.

## ğŸš€ Tecnologias Utilizadas

**Python 3.13+ |
[Requests](https://requests.readthedocs.io/en/latest/) |
[BeautifulSoup4](https://beautiful-soup-4.readthedocs.io/en/latest/#)**


## ğŸ¯ Objetivo

ImplementaÃ§Ã£o de um web crawler e tÃ©cnicas de web scraping, com o objetivo de coletar dados do site [Hacker News](https://news.ycombinator.com/), extraindo tÃ­tulo, autor, pontuaÃ§Ã£o e nÃºmero de comentÃ¡rios. 

### Crawler

Um web crawler Ã© um programa que navega automaticamente pela web, seguindo links entre pÃ¡ginas. Ele serve para descobrir URLs e mapear sites, coletando pÃ¡ginas para serem processadas depois.
SerÃ¡ usado para:

- Acessar a pÃ¡gina inicial;

- Capturar links de notÃ­cias;

- Navegar para a prÃ³xima pÃ¡gina (?p=2, ?p=3);

- Limitar para 3 pÃ¡ginas.

### Scraping

O web scraping Ã© o processo de extrair informaÃ§Ãµes especÃ­ficas de uma pÃ¡gina web jÃ¡ encontrada. SerÃ¡ usado para:

#### Para cada notÃ­cia, extrair:

ğŸ“° TÃ­tulo.

ğŸ”— Link.

ğŸ‘¤ Autor.

â­ PontuaÃ§Ã£o (score).

ğŸ’¬ NÃºmero de comentÃ¡rios.

#### Salvar tudo em json, exemplo:
```json
{  
  "title": "OpenAI launches new model",  
  "url": "https://...",  
  "author": "pg",  
  "points": 321,  
  "comments": 85  
} 
```
## âš™ï¸ Funcionalidades
- Crawler de mÃºltiplas pÃ¡ginas;
- ExtraÃ§Ã£o de dados estruturados;
- Salvamento em JSON;
- Tratamento bÃ¡sico de erros.

## ğŸš§ LimitaÃ§Ãµes
- O scraping depende da estrutura HTML do site;
- O projeto respeita limites bÃ¡sicos de requisiÃ§Ã£o.


## ğŸ“š Estrutura do Projeto
```
web_crawler_hn/  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ main.py           inicia o programa.
â”‚   â”œâ”€â”€ crawler.py        responsÃ¡vel por navegar entre pÃ¡ginas e coletar URLs.
â”‚   â”œâ”€â”€ scraper.py        responsÃ¡vel por extrair dados de cada pÃ¡gina.
â”‚   â””â”€â”€ storage.py        responsÃ¡vel por armazenar os dados.
â”œâ”€â”€ data/  
|   â””â”€â”€ news.json         local onde os dados estÃ£o armazenados.
â”œâ”€â”€ requirements.txt      requisitos para rodar o programa.
â””â”€â”€ README.md  
```
## ğŸ”§ Como Executar
```
python -m venv venv  
source venv/bin/activate  # Linux/macOS  
venv\Scripts\activate     # Windows  

pip install -r requirements.txt  
python src/main.py
```

## âš ï¸ Aviso
Este projeto Ã© apenas para fins educacionais.
Respeite os termos de uso dos sites coletados.